#!/bin/bash

# æŒ‡å®šä½¿ç”¨çš„ç‰©ç†æ˜¾å¡ ID
GPU_IDS="0,1,2,3,4,5,6,7"
NUM_GPUS=$(echo $GPU_IDS | tr -cd ',' | wc -c)
NUM_GPUS=$((NUM_GPUS + 1))

# DDP é€šä¿¡ç«¯å£ (å¦‚æœå¤šä»»åŠ¡åŒæ—¶è¿è¡Œï¼Œéœ€ä¿®æ”¹æ­¤ç«¯å£ä»¥é˜²å†²çª)
MASTER_PORT=29500

# é™åˆ¶ CPU çº¿ç¨‹æ•° (é˜²æ­¢æ•°æ®åŠ è½½æ—¶ CPU äº‰æŠ¢å¯¼è‡´è®­ç»ƒå¡æ­»)
export OMP_NUM_THREADS=4

# è®­ç»ƒç»“æœã€æ¨¡å‹ã€æ—¥å¿—ä¿å­˜ç›®å½•
CHECKPOINT_DIR="/Netdata/2025/wjc/checkpoints_kuochong_ddp_final"


# å•å¼ æ˜¾å¡çš„ Batch Size 
# å…¨å±€ Batch Size = BATCH_SIZE_PER_GPU * NUM_GPUS
BATCH_SIZE_PER_GPU=16
# æ€»è®­ç»ƒè½®æ¬¡
TOTAL_EPOCHS=30

# LMFT
# å‰ N è½®ä½¿ç”¨ Softmax (Margin=0)
WARMUP_EPOCHS=5
# å N è½®ä½¿ç”¨å®Œæ•´ Margin (Margin=0.5)
FINETUNE_EPOCHS=8

# æ˜¯å¦å¯ç”¨è¯­é€ŸéŸ³é«˜æ‰©å¢ (æ•°æ®é‡ x3)
# true: å¼€å¯ (æ¨èï¼Œæå‡é²æ£’æ€§)
# false: å…³é—­ (ä»…ä½¿ç”¨åŸå§‹æ•°æ®ï¼Œé€Ÿåº¦å¿«)
ENABLE_SPEED_PERTURB=true

# æ˜¯å¦å¯ç”¨ç¯å¢ƒå™ªå£°/æ··å“å¢å¼º
# true: å¼€å¯ (æ¨è)
# false: å…³é—­
ENABLE_ENV_AUGMENTATION=true


# è‡ªåŠ¨æ¢å¤: true/false
# å¦‚æœä¸º trueï¼Œè„šæœ¬ä¼šè‡ªåŠ¨å» CHECKPOINT_DIR æ‰¾æœ€æ–°çš„ .pth ç»§ç»­è®­ç»ƒ
AUTO_RESUME=false

# æŒ‡å®šæ¢å¤è·¯å¾„ (ä»…å½“ AUTO_RESUME=false æ—¶ç”Ÿæ•ˆ)
# å¦‚æœç•™ç©º ""ï¼Œä¸” AUTO_RESUME=falseï¼Œåˆ™ä»å¤´å¼€å§‹è®­ç»ƒ (Epoch 1)
SPECIFIC_RESUME_PATH=""
# ç¤ºä¾‹: SPECIFIC_RESUME_PATH="/path/to/best_model_epoch_10.pth"



# 1. è®¾ç½®å¯è§æ˜¾å¡
export CUDA_VISIBLE_DEVICES=$GPU_IDS

# 2. æ„å»º Python è¿è¡Œå‚æ•°
PY_ARGS="--checkpoint_dir ${CHECKPOINT_DIR} \
         --batch_size ${BATCH_SIZE_PER_GPU} \
         --epochs ${TOTAL_EPOCHS} \
         --warmup_epochs ${WARMUP_EPOCHS} \
         --fine_tune_epochs ${FINETUNE_EPOCHS}"

# å¤„ç†å¼€å…³é€»è¾‘
if [ "$ENABLE_SPEED_PERTURB" = true ]; then
    PY_ARGS="${PY_ARGS} --speed_perturb"
fi

if [ "$ENABLE_ENV_AUGMENTATION" = false ]; then
    # æ³¨æ„ï¼šPythonè„šæœ¬ä¸­æ˜¯ --disable_augï¼Œæ‰€ä»¥è¿™é‡Œåå‘é€»è¾‘
    PY_ARGS="${PY_ARGS} --disable_aug"
fi

if [ "$AUTO_RESUME" = true ]; then
    PY_ARGS="${PY_ARGS} --auto_resume"
elif [ -n "$SPECIFIC_RESUME_PATH" ]; then
    PY_ARGS="${PY_ARGS} --resume ${SPECIFIC_RESUME_PATH}"
fi

# 3. æ‰“å°è¿è¡Œä¿¡æ¯
echo "======================================================="
echo "   ğŸš€ å¯åŠ¨ DDP åˆ†å¸ƒå¼è®­ç»ƒ"
echo "======================================================="
echo "   - æ˜¾å¡åˆ—è¡¨    : $GPU_IDS (å…± $NUM_GPUS å¼ )"
echo "   - ä¿å­˜ç›®å½•    : $CHECKPOINT_DIR"
echo "   - å…¨å±€ Batch  : $((BATCH_SIZE_PER_GPU * NUM_GPUS)) (å•å¡: $BATCH_SIZE_PER_GPU)"
echo "   - æ€»è½®æ¬¡      : $TOTAL_EPOCHS (Warmup: $WARMUP_EPOCHS, Finetune: $FINETUNE_EPOCHS)"
echo "   - è¯­é€Ÿæ‰©å¢    : $ENABLE_SPEED_PERTURB"
echo "   - ç¯å¢ƒå¢å¼º    : $ENABLE_ENV_AUGMENTATION"
if [ "$AUTO_RESUME" = true ]; then
    echo "   - æ–­ç‚¹ç­–ç•¥    : è‡ªåŠ¨æ¢å¤ (Auto Resume)"
elif [ -n "$SPECIFIC_RESUME_PATH" ]; then
    echo "   - æ–­ç‚¹ç­–ç•¥    : æŒ‡å®šæ–‡ä»¶ -> $SPECIFIC_RESUME_PATH"
else
    echo "   - æ–­ç‚¹ç­–ç•¥    : âš ï¸ ä»å¤´å¼€å§‹è®­ç»ƒ (Fresh Start)"
fi
echo "======================================================="

# 4. æ‰§è¡ŒæŒ‡ä»¤
# ä½¿ç”¨ torchrun å¯åŠ¨
torchrun \
    --nproc_per_node=$NUM_GPUS \
    --master_port=$MASTER_PORT \
    train_ddp.py $PY_ARGS